---
title: "Methodology"
author: "Jessica Tan"
date: "7/25/2021"
output: distill::distill_article
---

# A. Data Preparation

The following is run to define the Global Settings for code chunks in this post:

```{r setup, include=TRUE}
knitr::opts_chunk$set(fig.retina=3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

## A(i) Installation and loading of packages:

The relevant packages required in analysis are installed as follows:

```{r, echo=TRUE, eval=TRUE}
packages = c('tidytext', 'igraph', 'extrafont',
             'tidygraph', 'ggraph', 'tcltk','anytime',
             'widyr', 'wordcloud', 'readxl', 'mgsub',
             'DT', 'ggwordcloud', 'LDAvis', 
             'textplot', 'tidyverse','lookup',
             'dplyr', 'tidyr','tm','quanteda', 
             'stringr', 'SnowballC','quanteda.textplots',
             'visNetwork','lubridate', 'reshape2',
             'RColorBrewer', 'htmltools', 'tidyr',
             'readr', 'purrr','clock',
             'corporaexplorer','stringr')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```


## A(ii) Import data

Function is created to read and collect all the news articles into a dataframe:

```{r eval=FALSE}
news <- "data/news/"
```


```{r eval=FALSE}
read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, 
                      read_lines)) %>%
    transmute(id = basename(file), 
              text) %>%
    unnest(text)
}
```


```{r eval= FALSE}
raw_text <- tibble(folder = 
                     dir(news, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), 
            id, text)

write_rds(raw_text, "data/rds/news.rds")

```

The created dataframe containing the news articles is then saved to rds file to minimize memory consumption:

```{r}
raw_text <- read_rds("data/rds/news.rds")
cleaning_text <- raw_text
```


## A(iii) Text Pre-processing

The news texts collected in dataframe is further processed to provide columns for title, date and location which would be extracted from the text column containing the news text:

```{r}
cleaning_text$date <- sub("PUBLISHED:", "", raw_text$text, 
                     ignore.case = TRUE, fixed = FALSE)
cleaning_text$date = anydate(cleaning_text$date)
dates <- cleaning_text[c(1,2,4)]
dates <- dates[complete.cases(dates),]
```


```{r}
cleaning_text$title <- str_extract(cleaning_text$text, "TITLE:.*")
cleaning_text$title <- gsub(paste0("TITLE:", collapse = "|"), "", cleaning_text$title)
title <- cleaning_text[c(1,2,5)]
title <- title[complete.cases(title),]  
```


```{r}
cleaning_text$location <- str_extract(cleaning_text$text, "LOCATION:.*")
cleaning_text$location <- gsub(paste0("LOCATION:", collapse = "|"), "", 
                               cleaning_text$location)
location <- cleaning_text[c(1,2,6)]
location <- location[complete.cases(location),]  

```


Note that although title column is created and copied from the text column, the text column still retains the title of the article for purposes of subsequent processing and analysis. However, location and date published would be removed after they were extracted into separate columns as these 2 details should not be included in the processing/analysis of the main news texts.

```{r}
cleaning_text <- cleaning_text[!(is.na(cleaning_text$text) |cleaning_text$text==""|
                         cleaning_text$text==" "),]
cleaning_text <- cleaning_text[!grepl("SOURCE:", cleaning_text$text),]
cleaning_text <- cleaning_text[!grepl("PUBLISHED:", cleaning_text$text),]
cleaning_text <- cleaning_text[!grepl("LOCATION:", cleaning_text$text),]
cleaning_text <- cleaning_text[is.na(cleaning_text$date),]
af_cleaning_text <- cleaning_text[c(1:3)] 
af_cleaning_text$title <- with(title, title[match(af_cleaning_text$id, id)])
af_cleaning_text$location <- with(location, location[match(af_cleaning_text$id, id)])
af_cleaning_text$date <- with(dates, date[match(af_cleaning_text$id, id)])
af_cleaning_text$year <- format(as.Date(af_cleaning_text$date, format="%d/%m/%Y"), "%Y")
after_clean_text <- af_cleaning_text
```


# 1. Characterize news sources

## 1.1 Frequency & Duration 

```{r echo=FALSE}
text_count <-after_clean_text %>%
  group_by(newsgroup) %>%
  summarize(value = n_distinct(id))
```

```{r, layout="l-body-outset", fig.width=10, fig.height=8}
text_count %>%
  mutate(newsgroup = reorder(newsgroup, value)) %>%
  ggplot(aes(value, newsgroup)) +
  geom_col(fill = "cornflowerblue") +
  labs(y = 'Newgroups', x='No. of Articles')+
  ggtitle("Frequency of News Articles by Newsgroup")
  
```


```{r, layout="l-body-outset", fig.width=10, fig.height=8}
ggplot(af_cleaning_text, 
       aes(x= as.numeric(year),
           fill=newsgroup)) +
  geom_bar()+
  labs(y = 'No. of Articles', x='Year')+
  ggtitle("Frequency of News Articles by Year")

  
```

## 1.2 Stopwords Removal & Stemming

```{r}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()
```

```{r}
cleaned_text <- af_cleaning_text %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )
```

Text tokenized and stopwords removed:

```{r}
usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

```

Count word stems:

```{r}
usenet_words %>%
  mutate(word=wordStem(word))%>%
  count(word, sort = TRUE)
```

Remove meaningless frequent words not previously removed via stopwords, i.e. title, published & kronos - so that they do not distort the subsequent text visualization:

```{r}
words_by_newsgroup <- usenet_words %>%
  filter(str_detect(word, "title")==FALSE)%>%
  filter(str_detect(word, "published")==FALSE)%>%
  filter(str_detect(word, "kronos")==FALSE)%>%
  count(newsgroup,id, word, sort = TRUE) %>%
  ungroup()
```

## 1.3 Detect similar sentiment profiles

```{r, layout="l-body-outset",  fig.width=10, fig.height=8}

newsgroup_sentiments <- words_by_newsgroup %>%
  mutate(word=wordStem(word))%>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(newsgroup) %>%
  summarize(value = sum(value * n) / sum(n))

newsgroup_sentiments %>%
  mutate(newsgroup = reorder(newsgroup, value)) %>%
  ggplot(aes(value, newsgroup, fill = value)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Average sentiment value", y = NULL)+
  ggtitle("News Sentiment Score using AFINN Sentiment Lexicon")
```

From above, there are indeed quite a few pairs/groups of newsgroups with similar text sentiment profiles, e.g. The Abila Post and Central Bulletin had same sentiment scores. Could these pairs/groups be posting common or similar news articles to result in the same scores? If so, which are the primary and derivative sources? We will try to investigate this in the following analysis.



## 1.4 Identify important words by newsgroup based on TFIDF

The following step is to identify the most important words for each newsgroup using tfidf:

```{r}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))

```


```{r echo=TRUE, eval=TRUE, layout="l-page", fig.width=16, fig.height=10}
tf_idf %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, n = 4) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, scales = "free")+
  scale_y_reordered()+
  labs(x = "tf-idf", y = NULL)

```

The above shows some newsgroups with common profiles in terms of TFIDF. This would be investigated further below by reviewing the correlation of the news articles.


## 1.5 Identify newsgroups with high correlation based on their articles

First, pairwise correlation is calculated for all news articles in each newsgroup:


```{r}
newsgroup_cors1 <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, 
               word, 
               n, 
               sort = TRUE)
```


Next, network graph is plotted with the newsgroups as nodes and the correlation scores of their news articles as edges:

```{r, layout="l-body-outset", fig.width=8, fig.height=6}
set.seed(2017)


newsgroup_cors1 %>%
  filter(correlation > .6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "cornflowerblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```

From above, we can observe that higher correlation between newsgroups is represented by the thickening and darkening of their edges with each other. For example, "The World" is at the centre of a sub-network of newsgroups and has expressedly stronger correlation with "Who What News". In another sub-network group, "The Abila Post" showed stronger text correlation with "Central Bulletin". 

In total, we observed 5 separate sub-newsgroup networks led by 5 core newsgroups which we can infer as the primary news sources since they acted like a hub and showed strong correlation of 0.7 or more with other newsgroups in their networks. The 5 primary news sources are (1) "The Abila", (2) "The World", (3) "International Times", (4) "Kronos Star", and (5) "Homeland Illumination". The other newsgroups surrounding these hubs are the derivative news sources.    


## 1.6 Further trace newsgroup correlation in the news articles

In the following, text wrangling is performed to obtain a datatable containing the correlated news article pairs with column details on text id, date, title and newsgroup for each of the paired news articles. We selected article pairs with high correlation of >0.8. 

```{r}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(id, 
               word, 
               n, 
               sort = TRUE)

art_cors <- newsgroup_cors
```

Obtain dates for the 1st article in each correlated news article pair:

```{r}
art_cors <- merge(x=art_cors,y=dates, 
                  by.x = "item1",                         
                  by.y = "id", 
                  all.x = TRUE, all.y = FALSE)

```

Obtain news title for 1st article:

```{r}
art_cors <- merge(x=art_cors,y=title, 
                  by.x = "item1",                         
                  by.y = "id", 
                  all.x = TRUE, all.y = FALSE)
```

Column names are corrected by pasting over:

```{r}
names(art_cors)[1] <- paste("article1")
names(art_cors)[2] <- paste("article2")
names(art_cors)[4] <- paste("newsgroup1")
names(art_cors)[5] <- paste("date1")
names(art_cors)[7] <- paste("title1")
art_cors1 <- art_cors[-c(6)]
```

The same process (as 1st article mentioned above) for 2nd article, starting with title for 2nd article:

```{r}
art_cors1 <- merge(x=art_cors1,y=title, 
                  by.x = "article2",                         
                  by.y = "id", 
                  all.x = TRUE)
```

Retrieve date for 2nd article:

```{r}
art_cors2 <- merge(x=art_cors1,y=dates, 
                  by.x = "article2",                         
                  by.y = "id", 
                  all.x = TRUE, all.y = FALSE)
```

Correct column titles by pasting over:

```{r}
names(art_cors2)[7] <- paste("newsgroup2")
names(art_cors2)[8] <- paste("title2")
names(art_cors2)[10] <- paste("date2")
art_cors_final <- art_cors2[-c(9)]
art_cors_final <- art_cors_final%>%
  filter(correlation >0.8)
```

Remove unnecessary columns resulting from the merging of tables:

```{r}
art_cors_table <- art_cors_final[-c(6,8)]
art_cors_table <- art_cors_table%>%
  filter(correlation >0.8)
art_cors_table1 <- art_cors_table[, c(2,4,5,1,6,7,3)]
art_cors_table2 <- art_cors_table1[, c(1,4,7)]
```

```{r eval=FALSE}
articles <- cleaned_text[, c(2,3)]
```

Retrieving text body for 1st article:

```{r eval=FALSE}
art_cors_table2 <-  merge(x=art_cors_table2,y=articles, 
                          by.x = "article1",                         
                          by.y = "id", 
                          all.x = TRUE)
names(art_cors_table2)[4] <- paste("text1")
```

Retrieving text body for second article:

```{r eval=FALSE}
art_cors_table3 <-  merge(x=art_cors_table2,y=articles, 
                          by.x = "article2",                         
                          by.y = "id", 
                          all.x = TRUE)
names(art_cors_table3)[5] <- paste("text2")
```


Finally, the datatable for the correlated news article pairs is set up as follow:

```{r, layout="l-body-outset", fig.width=14, fig.height=8}
DT::datatable(art_cors_table1,
              rownames = FALSE,
              options = list(pageLength=8),
              filter = 'top')%>%
  formatRound(columns = c('correlation'),
              digits = 4)%>%
  formatStyle(0, target = "row",
              lineHeight = "85%")
```

With the above datatable, we would be able to identify the primary and derivative news sources for each highly correlated news article pairs (>0.8 correlation) by comparing their published dates. The article with the earlier date would be the primary source as the later article would essentially be obtaining its text from the primary article after the primary article has been published.


# 2. Detect & characterize biases in news sources


## 2.1 Perform wordcloud using key words obtained in above tfidf:
```{r}
totalwords <- words_by_newsgroup %>%
  count(word, sort=TRUE)
```
```{r, layout="l-body-outset", fig.width=10, fig.height=10}
set.seed(1234)
wordcloud(totalwords$word,totalwords$n,max.words = 115,
          colors = brewer.pal(8, "Dark2"))
```
The wordcloud highlighted the issues such as death, contamination, kidnapping, as well as the names of the key parties including Juliana Vann who died from illness caused by the river contamination, GASTech's CEO, Sten Sanjorge, POK, Government etc.


## 2.2 Identify associated sentiment words in news reporting

To identify news bias, the important sentiment words used in the news articles are retrieved and sentiment-scored (via AFINN).

```{r}
top_sentiment_words <- words_by_newsgroup %>%
  mutate(word=wordStem(word))%>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  mutate(sentiment = value * n / sum(n))

```

The outcome is presented in the form of the following datatable for viewing the sentiment words used in the articles, the frequency used, and their corresponding sentiment values:

```{r, layout="l-body-outset", fig.width=12, fig.height=10}

DT::datatable(top_sentiment_words, rownames = FALSE,
              options = list(pageLength=8), 
              filter = 'top')%>% 
  formatRound('value',4) %>%
  formatRound('sentiment',4) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='90%')
```

## 2.3 Compute & apply word bigram in news sentiment analysis

The word bigrams are calculated for the news articles
```{r}
usenet_bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

```

```{r}
usenet_bigram_counts <- usenet_bigrams %>%
  count(newsgroup, bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

```

## 2.4 Sentiment scoring of associated word bigrams - key people

The following plots the high frequency sentiment words used in news articles to surround these names (i.e. key individuals for in POK, GasTech and Kronos Government):

```{r, layout="l-body-outset", fig.width=10, fig.height=12}
key_persons <- c("sanjorge", "carmine", "nespola", "marek", 
                 "bodrogi", "jeroen","juliana","kapelou", "elian")

usenet_bigram_counts %>%
  filter(word1 %in% key_persons) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 5) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 6) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with key persons")
```

It can be observed key POK personnel Bodrogi and Carmine elicited positive sentiment scores whilst others such as Elian and Juliana had negative sentiment scores resulting from the occurrence of their deaths. Government officials, Nespola and President Kapelou as well as GASTech CEO Sanjorge also have negative scores resulting from negative words frequenly associated with their names in news articles. Marek, a POK member, also drew negative sentiment score due to the word "stop", a word for obstacle.This will be investigated in the news texts later.  


```{r, layout="l-body-outset", fig.width=10, fig.height=10}

key_entities <- c("pok", "government", "gastech", "wfa")

usenet_bigram_counts %>%
  filter(word1 %in% key_entities) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 15) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with key entities")
```

The above shows the most important sentiment words associated with the key players - GASTech, POK, Government and WFA. WFA, being a non-profit NGO, has positive sentiment scores during to its supportive role. The Government and POK have fairly balanced spread of positive and negative sentiment words due to their supportive roles in their communities as well as negative sentiment words due to their actions which had negative connotations. In comparison, GASTech is skewed towards negative sentiment words due to the issues of contamination surrounding their activities which purported caused harm to the lives of the people.  


```{r, layout="l-body-outset", fig.width=10, fig.height=10}

key_places <- c("tiskele", "elodis", "kronos","tethys", 
                "abila", "rural", "city", "fields", "port", "pilau")

usenet_bigram_counts %>%
  filter(word1 %in% key_places) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 5) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with important places")

```
Similarly, the above places have varying sentiment scores due to the events reported in association with these places. For example, the numerous protests that occurred in Elodis results in a negatively skewed sentiment score whilst the help provided to the rural areas in positive sentiment score.


```{r,layout="l-body-outset", fig.width=10, fig.height=10}

key_places <- c("contamination", "protests", "kidnapping", "death","arrested",
                "disease", "movement", "alliance")

usenet_bigram_counts %>%
  filter(word1 %in% key_places) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 3) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with key events")
```

For events reported, the sentiment scores seem to be correct for most, i.e. negative sentiment associated with arrests, deaths, protests etc. However, for events such as disease and contamination, the associated words such as "responsible" and "increased" (which are classified by AFINN as positive sentiment words) may have incorrectly tilted the sentiment scores in the positive direction.  



```{r}
raw_emails <- read_csv("data/email headers.csv")

```

```{r}
employee_rec <- read_excel("data/EmployeeRecords.xlsx")

```

```{r echo=TRUE, eval=TRUE}
raw_emails$Date <- date_time_parse(raw_emails$Date,
                                   zone = "",
                                   format = "%m/%d/%Y %H:%M")
```


```{r echo=TRUE, eval=TRUE}
raw_emails$Weekday = wday(raw_emails$Date, 
                             label = TRUE, 
                             abbr = FALSE)

```


```{r echo=TRUE, eval=TRUE}
cleaned_emails <- raw_emails%>%
  mutate(To = str_remove_all(To, ","))%>%
  mutate(To = str_remove_all(To, From))%>%
  mutate(To = str_remove_all(To, "@gastech.com.kronos"))%>%
  mutate(To = str_remove_all(To, "@gastech.com.tethys"))

```

  mutate(To = str_remove_all(To, " "))%>%
```{r echo=TRUE, eval=TRUE}
cleaned_emails <- cleaned_emails%>%
  mutate(From = str_remove_all(From, "@gastech.com.kronos"))%>%
  mutate(From = str_remove_all(From, "@gastech.com.tethys"))
```

```{r echo=TRUE, eval=TRUE}
employee_rec <- employee_rec %>% 
  mutate(EmailAddress = str_remove_all(EmailAddress, "@gastech.com.kronos"))%>%
  mutate(EmailAddress = str_remove_all(EmailAddress, "@gastech.com.tethys"))

```

##Generate id_From for sender, and id_To for receiver in emails:
```{r echo=TRUE, eval=TRUE}
cleaned_emails <- transform(cleaned_emails, id_From 
                            =as.numeric(factor(cleaned_emails$From)))

```


```{r echo=TRUE, eval=TRUE}
cleaned_emails <- cleaned_emails %>% 
  separate(To, c("To_1", "To_2", "To_3", "To_4", "To_5", "To_6", "To_7", 
                 "To_8", "To_9", "To_10", "To_11", "To_12"), " ")

```



```{r}
email_transpose <- pivot_longer(cleaned_emails, cols=2:13, 
                                 names_to = "Recipient#",
                                 values_to = "To")
```

```{r}
email_transpose <- email_transpose[complete.cases(email_transpose),]
email_transpose <- email_transpose[!(email_transpose$To == 
                                       ""|email_transpose$To=="Jr."),]    
email_transpose$To[email_transpose$To=="Sten.Sanjorge"] <- "Sten.Sanjorge Jr."
```



```{r}
nodes <- email_transpose[c("id_From","From")]
nodes <- nodes%>% distinct()
names(nodes)[1] <- paste("id")
names(nodes)[2] <- paste("Name")

```

```{r echo=TRUE, eval=TRUE}
email_transpose <- merge(x = nodes, y = email_transpose, 
               by.x = "Name", 
               by.y = "To", all.x = TRUE)

```


```{r echo=TRUE, eval=TRUE}
names(email_transpose)[1] <- paste("To")
names(email_transpose)[2] <- paste("id_To")
```

```{r echo=TRUE, eval=TRUE}
GASTech_nodes <- merge(x= nodes, y= employee_rec, 
                       by.x = "Name", by.y="EmailAddress", all.x = FALSE)

```

```{r echo=TRUE, eval=TRUE}
GAStech_edges_aggregated <- email_transpose %>%
  group_by(id_From, id_To, Weekday) %>%
  summarise(Weight = n()) %>%
  filter(id_From!=id_To) %>%
  filter(Weight > 1) %>%
  ungroup()

```


```{r echo=FALSE, eval=TRUE}
glimpse(GAStech_edges_aggregated)
```





```{r, echo=TRUE, eval=TRUE}
GAStech_graph <- tbl_graph(nodes = GASTech_nodes,
                           edges = GAStech_edges_aggregated, 
                           directed = TRUE)
```

 


```{r}
GAStech_graph
```

```{r, layout="l-body-outset", fig.width=14, fig.height=11}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = Name, 
                      size = centrality_betweenness()))+
  ggtitle("Nodes & Edges of Email Network - Key Individuals")
g + theme_graph()
```


```{r echo=FALSE}
GASTech_nodes_selected <- GASTech_nodes[,c(1,14, 15,8,17)]


```


```{r, layout="l-body-outset", fig.width=8, fig.height=10}
DT::datatable(GASTech_nodes_selected, 
              colnames = c("Department"=3, "Position"=4,"Country"=5,
                           "Military Service"=6),
              options = list(pageLength=7), 
              filter = 'top')%>%
  formatStyle(0, 
              target = 'row',
              lineHeight="95%")
```


```{r, layout="l-body-outset", fig.width=10, fig.height=6}
set_graph_style() 

g <- ggraph(GAStech_graph, 
            layout = "nicely") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = CurrentEmploymentType), 
                  size = 2)+
  ggtitle("Nodes & Edges of Email Network - Day of Week")
  
g + facet_edges(~Weekday) +
  th_foreground(foreground = "grey80",  
                border = TRUE) +
  theme(legend.position = 'bottom')
```


```{r, layout="l-body-outset", fig.width=12, fig.height=8}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = CurrentEmploymentTitle, 
                      size = centrality_betweenness()))+
  ggtitle("Nodes & Edges of Email Network - Job Holders")
g + theme_graph()
```


```{r, layout="l-body-outset",fig.width=10, fig.width=8}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = CitizenshipCountry, 
                      size = centrality_betweenness()))+
  ggtitle("Email Network - Citizenship Country")

g + theme_graph()
```


```{r, layout="l-body-outset", fig.width=10, fig.height=8}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = MilitaryServiceBranch, 
                      size = centrality_betweenness()))+
  ggtitle("Nodes & Edges of Email Network - Military Service Branch")

g + theme_graph()
```
