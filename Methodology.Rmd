---
title: "Methodology"
author: "Jessica Tan"
date: "7/25/2021"
output: distill::distill_article
---

# A. Data Preparation

The following is run to define the Global Settings for code chunks in this post:

```{r setup, include=TRUE}
knitr::opts_chunk$set(fig.retina=3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

## A(i) Installation and loading of packages:

The relevant packages required in analysis are installed as follows:

```{r, echo=TRUE, eval=TRUE}
packages = c('tidytext', 'igraph', 'extrafont',
             'tidygraph', 'ggraph', 'tcltk','anytime',
             'widyr', 'wordcloud', 'readxl', 'mgsub',
             'DT', 'ggwordcloud', 'LDAvis', 
             'textplot', 'tidyverse','lookup',
             'dplyr', 'tidyr','tm','quanteda', 
             'stringr', 'SnowballC','quanteda.textplots',
             'visNetwork','lubridate', 'reshape2',
             'RColorBrewer', 'htmltools', 'tidyr',
             'readr', 'purrr','clock',
             'corporaexplorer','stringr')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```


## A(ii) Import data

Function is created to read and collect all the news articles into a dataframe:

```{r eval=FALSE}
news <- "data/news/"
```


```{r eval=FALSE}
read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, 
                      read_lines)) %>%
    transmute(id = basename(file), 
              text) %>%
    unnest(text)
}
```


```{r eval= FALSE}
raw_text <- tibble(folder = 
                     dir(news, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), 
            id, text)

write_rds(raw_text, "data/rds/news.rds")

```

The created dataframe containing the news articles is then saved to rds file to minimize memory consumption:

```{r}
raw_text <- read_rds("data/rds/news.rds")
cleaning_text <- raw_text
```


## A(iii) Text Pre-processing

The news texts collected in dataframe is further processed to provide columns for title, date and location which would be extracted from the text column containing the news text:

```{r}
cleaning_text$date <- sub("PUBLISHED:", "", raw_text$text, 
                     ignore.case = TRUE, fixed = FALSE)
cleaning_text$date = anydate(cleaning_text$date)
dates <- cleaning_text[c(1,2,4)]
dates <- dates[complete.cases(dates),]
```


```{r}
cleaning_text$title <- str_extract(cleaning_text$text, "TITLE:.*")
cleaning_text$title <- gsub(paste0("TITLE:", collapse = "|"), "", cleaning_text$title)
title <- cleaning_text[c(1,2,5)]
title <- title[complete.cases(title),]  
```


```{r}
cleaning_text$location <- str_extract(cleaning_text$text, "LOCATION:.*")
cleaning_text$location <- gsub(paste0("LOCATION:", collapse = "|"), "", 
                               cleaning_text$location)
location <- cleaning_text[c(1,2,6)]
location <- location[complete.cases(location),]  

```


Note that although title column is created and copied from the text column, the text column still retains the title of the article for purposes of subsequent processing and analysis. However, location and date published would be removed after they were extracted into separate columns as these 2 details should not be included in the processing/analysis of the main news texts.

```{r}
cleaning_text <- cleaning_text[!(is.na(cleaning_text$text) |cleaning_text$text==""|
                         cleaning_text$text==" "),]
cleaning_text <- cleaning_text[!grepl("SOURCE:", cleaning_text$text),]
cleaning_text <- cleaning_text[!grepl("PUBLISHED:", cleaning_text$text),]
cleaning_text <- cleaning_text[!grepl("LOCATION:", cleaning_text$text),]
cleaning_text <- cleaning_text[is.na(cleaning_text$date),]
af_cleaning_text <- cleaning_text[c(1:3)] 
af_cleaning_text$title <- with(title, title[match(af_cleaning_text$id, id)])
af_cleaning_text$location <- with(location, location[match(af_cleaning_text$id, id)])
af_cleaning_text$date <- with(dates, date[match(af_cleaning_text$id, id)])
af_cleaning_text$year <- format(as.Date(af_cleaning_text$date, format="%d/%m/%Y"), "%Y")
after_clean_text <- af_cleaning_text
```


# 1. Characterize news sources

## 1.1 Frequency & Duration 

```{r echo=FALSE}
text_count <-after_clean_text %>%
  group_by(newsgroup) %>%
  summarize(value = n_distinct(id))
```

```{r, layout="l-body-outset", fig.width=10, fig.height=8}
text_count %>%
  mutate(newsgroup = reorder(newsgroup, value)) %>%
  ggplot(aes(value, newsgroup)) +
  geom_col(fill = "cornflowerblue") +
  labs(y = 'Newgroups', x='No. of Articles')+
  ggtitle("Frequency of News Articles by Newsgroup")
  
```


```{r, layout="l-body-outset", fig.width=10, fig.height=8}
ggplot(af_cleaning_text, 
       aes(x= as.numeric(year),
           fill=newsgroup)) +
  geom_bar()+
  labs(y = 'No. of Articles', x='Year')+
  ggtitle("Frequency of News Articles by Year")

  
```

## 1.2 Stopwords Removal & Stemming

```{r}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()
```

```{r}
cleaned_text <- af_cleaning_text %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )
```

Text tokenized and stopwords removed:

```{r}
usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

```

Count word stems:

```{r}
usenet_words %>%
  mutate(word=wordStem(word))%>%
  count(word, sort = TRUE)
```

Remove meaningless frequent words not previously removed via stopwords, i.e. title, published & kronos - so that they do not distort the subsequent text visualization:

```{r}
words_by_newsgroup <- usenet_words %>%
  filter(str_detect(word, "title")==FALSE)%>%
  filter(str_detect(word, "published")==FALSE)%>%
  filter(str_detect(word, "kronos")==FALSE)%>%
  count(newsgroup,id, word, sort = TRUE) %>%
  ungroup()
```

## 1.3 Detect similar sentiment profiles

```{r, layout="l-body-outset",  fig.width=10, fig.height=8}

newsgroup_sentiments <- words_by_newsgroup %>%
  mutate(word=wordStem(word))%>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(newsgroup) %>%
  summarize(value = sum(value * n) / sum(n))

newsgroup_sentiments %>%
  mutate(newsgroup = reorder(newsgroup, value)) %>%
  ggplot(aes(value, newsgroup, fill = value)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Average sentiment value", y = NULL)+
  ggtitle("News Sentiment Score using AFINN Sentiment Lexicon")
```

From above, there are indeed quite a few pairs/groups of newsgroups with similar text sentiment profiles, e.g. The Abila Post and Central Bulletin had same sentiment scores. Could these pairs/groups be posting common or similar news articles to result in the same scores? If so, which are the primary and derivative sources? We will try to investigate this in the following analysis.



## 1.5 Identify important words by newsgroup based on TFIDF

The following step is to identify the most important words for each newsgroup using tfidf:

```{r}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))

```


```{r echo=TRUE, eval=TRUE, layout="l-page", fig.width=16, fig.height=10}
tf_idf %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, n = 4) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, scales = "free")+
  scale_y_reordered()+
  labs(x = "tf-idf", y = NULL)

```

The above shows some newsgroups with common profiles in terms of TFIDF. This would be investigated further below by reviewing the correlation of the news articles.


## 1.5 Identify newsgroups with high correlation based on their articles

First, pairwise correlation is calculated for all news articles in each newsgroup:


```{r}
newsgroup_cors1 <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, 
               word, 
               n, 
               sort = TRUE)
```

```{r, layout="l-body-outset", fig.width=8, fig.height=6}
set.seed(2017)


newsgroup_cors1 %>%
  filter(correlation > .6) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "cornflowerblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```

The following network graph is plotted with the newsgroups as nodes and the correlation scores of their news articles as edges. We can observe that higher correlation between newsgroups is represented by the thickening and darkening of their edges with each other. For example, "The World" is in the centre of a sub-network of newsgroups and has expressedly stronger correlation with "Who What News". In another sub-network group, "The Abila Post" showed stronger text correlation with "Central Bulletin". 

In total, we observed 5 separate sub-newsgroup network led by 5 core newsgroup which we can infer as the primary news sources since they acted like a hub and showed strong correlation of 0.7 or more with other newsgroups in their hubs. The 5 primary news sources are (1) "The Abila", (2) "The World", (3) "International Times", (4) "Kronos Star", and (5) "Homeland Illumination". The other newsgroups within these hubs are the derivative news sources.    


## 1.6 Further trace newsgroup correlation in their news articles

In the following, text wrangling is done to obtain a datatable on news article pairs with column details on txtid, date, title and newsgroup for each pair of correlated news articles. We selected article pairs with high correlation of >0.8. 

```{r}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(id, 
               word, 
               n, 
               sort = TRUE)

art_cors <- newsgroup_cors
```

Each pair of correlated article is merged into a common dataframe with column details on text id, dates, titles, newsgroups and correlation score. Data wrangling to extract and merge these details are as follow:

```{r}
art_cors <- merge(x=art_cors,y=dates, 
                  by.x = "item1",                         
                  by.y = "id", 
                  all.x = TRUE, all.y = FALSE)

```

```{r}
art_cors <- merge(x=art_cors,y=title, 
                  by.x = "item1",                         
                  by.y = "id", 
                  all.x = TRUE, all.y = FALSE)

```

Column names are corrected by pasting over:

```{r}
names(art_cors)[1] <- paste("article1")
names(art_cors)[2] <- paste("article2")
names(art_cors)[4] <- paste("newsgroup1")
names(art_cors)[5] <- paste("date1")
names(art_cors)[7] <- paste("title1")
art_cors1 <- art_cors[-c(6)]
```


```{r}
art_cors1 <- merge(x=art_cors1,y=title, 
                  by.x = "article2",                         
                  by.y = "id", 
                  all.x = TRUE)

```


```{r}
art_cors2 <- merge(x=art_cors1,y=dates, 
                  by.x = "article2",                         
                  by.y = "id", 
                  all.x = TRUE, all.y = FALSE)

```

To prepare for subsequent visualization, we select only correlated articles more than 0.8:

```{r}
names(art_cors2)[7] <- paste("newsgroup2")
names(art_cors2)[8] <- paste("title2")
names(art_cors2)[10] <- paste("date2")
art_cors_final <- art_cors2[-c(9)]
art_cors_final <- art_cors_final%>%
  filter(correlation >0.8)
```

To provide dataframe on news articles pairs, we extract and merge relevant columns for each of the 2 correlated news articles - txt id, newsgroup, date and correlation score. Below is for article 1:

```{r}
art_cors_table <- art_cors_final[-c(6,8)]
art_cors_table <- art_cors_table%>%
  filter(correlation >0.8)
art_cors_table1 <- art_cors_table[, c(2,4,5,1,6,7,3)]
art_cors_table2 <- art_cors_table1[, c(1,4,7)]
```

```{r eval=FALSE}
articles <- cleaned_text[, c(2,3)]

```

```{r eval=FALSE}
art_cors_table2 <-  merge(x=art_cors_table2,y=articles, 
                          by.x = "article1",                         
                          by.y = "id", 
                          all.x = TRUE)
names(art_cors_table2)[4] <- paste("text1")
```

Below is for article 2 of each correlated pair of news articles:

```{r eval=FALSE}
art_cors_table3 <-  merge(x=art_cors_table2,y=articles, 
                          by.x = "article2",                         
                          by.y = "id", 
                          all.x = TRUE)
names(art_cors_table3)[5] <- paste("text2")

```


Finally, the datatable for each correlated (>0.8) pair of new articles is ready below for checking and retrieval:

```{r, layout="l-body-outset", fig.width=14, fig.height=8}
DT::datatable(art_cors_table1,
              rownames = FALSE,
              options = list(pageLength=8),
              filter = 'top')%>%
  formatRound(columns = c('correlation'),
              digits = 4)%>%
  formatStyle(0, target = "row",
              lineHeight = "85%")

```

Using the above datatable, we would be able to identify highly correlated news articles as well as their primary and derivatives sources going by the published dates. The article with the earlier date in each pair would be the primary source as the later article would essentially be obtaining its text from the earlier article.



# 2. Detect & characterize biases in news articles 

## 2.1 Obtain frequency of important words to perform word cloud:
```{r}
totalwords <- words_by_newsgroup %>%
  count(word, sort=TRUE)

```

The following wordcloud shows the important words in the news articles based on tfidf:

```{r, layout="l-body-outset", fig.width=10, fig.height=10}
set.seed(1234)
wordcloud(totalwords$word,totalwords$n,max.words = 115,
          colors = brewer.pal(8, "Dark2"))

```

The wordcloud the names of the key players and parties including Juliana Vann who died from illness caused by the river contamination, as well as GASTech's CEO, Sten Sanjorge, POK, Government etc.

We further retrieve the top sentiment words and their scores using AFINN (sentiment words dictionary). This would first be presented in a datatable for ease of retrieval and checking against the sentiment words which would be subsequently plotted for "specific people, places, and events" (in response to Qn 2 of MC1).

```{r}
top_sentiment_words <- words_by_newsgroup %>%
  mutate(word=wordStem(word))%>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  mutate(contribution = value * n / sum(n))

```

```{r, layout="l-body-outset", fig.width=12, fig.height=10}

DT::datatable(top_sentiment_words, rownames = FALSE,
              options = list(pageLength=8), 
              filter = 'top')%>% 
  formatRound('value',4) %>%
  formatRound('contribution',4) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='90%')
```


```{r}
usenet_bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

```

```{r}
usenet_bigram_counts <- usenet_bigrams %>%
  count(newsgroup, bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

```

The following plots the high frequency sentiment words used in news articles to surround these names (i.e. key individuals for in POK, GasTech and Kronos Government):

```{r, layout="l-body-outset", fig.width=10, fig.height=12}
key_persons <- c("sanjorge", "carmine", "nespola", "marek", 
                 "bodrogi", "jeroen","juliana","kapelou", "elian")

usenet_bigram_counts %>%
  filter(word1 %in% key_persons) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 5) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 6) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with key persons")
```

It can be observed key POK personnel Bodrogi and Carmine elicited positive sentiment scores whilst others such as Elian and Juliana had negative sentiment scores resulting from the occurrence of their deaths. Government officials, Nespola and President Kapelou as well as GASTech CEO Sanjorge also have negative scores resulting from negative words frequenly associated with their names in news articles. Marek, a POK member, also drew negative sentiment score due to the word "stop", a word for obstacle.This will be investigated in the news texts later.  


```{r, layout="l-body-outset", fig.width=10, fig.height=10}

key_entities <- c("pok", "government", "gastech", "wfa")

usenet_bigram_counts %>%
  filter(word1 %in% key_entities) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 15) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with key entities")
```

The above shows the most important sentiment words associated with the key players - GASTech, POK, Government and WFA. WFA, being a non-profit NGO, has positive sentiment scores during to its supportive role. The Government and POK have fairly balanced spread of positive and negative sentiment words due to their supportive roles in their communities as well as negative sentiment words due to their actions which had negative connotations. In comparison, GASTech is skewed towards negative sentiment words due to the issues of contamination surrounding their activities which purported caused harm to the lives of the people.  


```{r, layout="l-body-outset", fig.width=10, fig.height=10}

key_places <- c("tiskele", "elodis", "kronos","tethys", 
                "abila", "rural", "city", "fields", "port", "pilau")

usenet_bigram_counts %>%
  filter(word1 %in% key_places) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 5) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with important places")

```
Similarly, the above places have varying sentiment scores due to the events reported in association with these places. For example, the numerous protests that occurred in Elodis results in a negatively skewed sentiment score whilst the help provided to the rural areas in positive sentiment score.


```{r,layout="l-body-outset", fig.width=10, fig.height=10}

key_places <- c("contamination", "protests", "kidnapping", "death","arrested",
                "disease", "movement", "alliance")

usenet_bigram_counts %>%
  filter(word1 %in% key_places) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 3) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "Sentiment value of occurrences",
       y = "Words associated with key events")
```

For events reported, the sentiment scores seem to be correct for most, i.e. negative sentiment associated with arrests, deaths, protests etc. However, for events such as disease and contamination, the associated words such as "responsible" and "increased" (which are classified by AFINN as positive sentiment words) may have incorrectly tilted the sentiment scores in the positive direction.  



```{r}
raw_emails <- read_csv("data/email headers.csv")

```

```{r}
employee_rec <- read_excel("data/EmployeeRecords.xlsx")

```

```{r echo=TRUE, eval=TRUE}
raw_emails$Date <- date_time_parse(raw_emails$Date,
                                   zone = "",
                                   format = "%m/%d/%Y %H:%M")
```


```{r echo=TRUE, eval=TRUE}
raw_emails$Weekday = wday(raw_emails$Date, 
                             label = TRUE, 
                             abbr = FALSE)

```


```{r echo=TRUE, eval=TRUE}
cleaned_emails <- raw_emails%>%
  mutate(To = str_remove_all(To, ","))%>%
  mutate(To = str_remove_all(To, From))%>%
  mutate(To = str_remove_all(To, "@gastech.com.kronos"))%>%
  mutate(To = str_remove_all(To, "@gastech.com.tethys"))

```

  mutate(To = str_remove_all(To, " "))%>%
```{r echo=TRUE, eval=TRUE}
cleaned_emails <- cleaned_emails%>%
  mutate(From = str_remove_all(From, "@gastech.com.kronos"))%>%
  mutate(From = str_remove_all(From, "@gastech.com.tethys"))
```

```{r echo=TRUE, eval=TRUE}
employee_rec <- employee_rec %>% 
  mutate(EmailAddress = str_remove_all(EmailAddress, "@gastech.com.kronos"))%>%
  mutate(EmailAddress = str_remove_all(EmailAddress, "@gastech.com.tethys"))

```

##Generate id_From for sender, and id_To for receiver in emails:
```{r echo=TRUE, eval=TRUE}
cleaned_emails <- transform(cleaned_emails, id_From 
                            =as.numeric(factor(cleaned_emails$From)))

```


```{r echo=TRUE, eval=TRUE}
cleaned_emails <- cleaned_emails %>% 
  separate(To, c("To_1", "To_2", "To_3", "To_4", "To_5", "To_6", "To_7", 
                 "To_8", "To_9", "To_10", "To_11", "To_12"), " ")

```



```{r}
email_transpose <- pivot_longer(cleaned_emails, cols=2:13, 
                                 names_to = "Recipient#",
                                 values_to = "To")
```

```{r}
email_transpose <- email_transpose[complete.cases(email_transpose),]
email_transpose <- email_transpose[!(email_transpose$To == 
                                       ""|email_transpose$To=="Jr."),]    
email_transpose$To[email_transpose$To=="Sten.Sanjorge"] <- "Sten.Sanjorge Jr."
```



```{r}
nodes <- email_transpose[c("id_From","From")]
nodes <- nodes%>% distinct()
names(nodes)[1] <- paste("id")
names(nodes)[2] <- paste("Name")

```

```{r echo=TRUE, eval=TRUE}
email_transpose <- merge(x = nodes, y = email_transpose, 
               by.x = "Name", 
               by.y = "To", all.x = TRUE)

```


```{r echo=TRUE, eval=TRUE}
names(email_transpose)[1] <- paste("To")
names(email_transpose)[2] <- paste("id_To")
```

```{r echo=TRUE, eval=TRUE}
GASTech_nodes <- merge(x= nodes, y= employee_rec, 
                       by.x = "Name", by.y="EmailAddress", all.x = FALSE)

```

```{r echo=TRUE, eval=TRUE}
GAStech_edges_aggregated <- email_transpose %>%
  group_by(id_From, id_To, Weekday) %>%
  summarise(Weight = n()) %>%
  filter(id_From!=id_To) %>%
  filter(Weight > 1) %>%
  ungroup()

```


```{r echo=FALSE, eval=TRUE}
glimpse(GAStech_edges_aggregated)
```





```{r, echo=TRUE, eval=TRUE}
GAStech_graph <- tbl_graph(nodes = GASTech_nodes,
                           edges = GAStech_edges_aggregated, 
                           directed = TRUE)
```

 


```{r}
GAStech_graph
```

```{r, layout="l-body-outset", fig.width=14, fig.height=11}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = Name, 
                      size = centrality_betweenness()))+
  ggtitle("Nodes & Edges of Email Network - Key Individuals")
g + theme_graph()
```


```{r echo=FALSE}
GASTech_nodes_selected <- GASTech_nodes[,c(1,14, 15,8,17)]


```


```{r, layout="l-body-outset", fig.width=8, fig.height=10}
DT::datatable(GASTech_nodes_selected, 
              colnames = c("Department"=3, "Position"=4,"Country"=5,
                           "Military Service"=6),
              options = list(pageLength=7), 
              filter = 'top')%>%
  formatStyle(0, 
              target = 'row',
              lineHeight="95%")
```


```{r, layout="l-body-outset", fig.width=10, fig.height=6}
set_graph_style() 

g <- ggraph(GAStech_graph, 
            layout = "nicely") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = CurrentEmploymentType), 
                  size = 2)+
  ggtitle("Nodes & Edges of Email Network - Day of Week")
  
g + facet_edges(~Weekday) +
  th_foreground(foreground = "grey80",  
                border = TRUE) +
  theme(legend.position = 'bottom')
```


```{r, layout="l-body-outset", fig.width=12, fig.height=8}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = CurrentEmploymentTitle, 
                      size = centrality_betweenness()))+
  ggtitle("Nodes & Edges of Email Network - Job Holders")
g + theme_graph()
```


```{r, layout="l-body-outset",fig.width=10, fig.width=8}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = CitizenshipCountry, 
                      size = centrality_betweenness()))+
  ggtitle("Email Network - Citizenship Country")

g + theme_graph()
```


```{r, layout="l-body-outset", fig.width=10, fig.height=8}
g <- GAStech_graph %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width=Weight), 
                 alpha=0.2) +
  scale_edge_width(range = c(0.1, 5)) +
  geom_node_point(aes(colour = MilitaryServiceBranch, 
                      size = centrality_betweenness()))+
  ggtitle("Nodes & Edges of Email Network - Military Service Branch")

g + theme_graph()
```
